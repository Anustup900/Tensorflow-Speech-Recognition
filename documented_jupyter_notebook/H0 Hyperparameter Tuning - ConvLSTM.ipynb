{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H0 Hyperparameter Tuning - ResConvLSTM \n",
    "#### Author: Jayant Verma\n",
    "#### Cognibit Solutions LLP\n",
    "\n",
    "Derived from https://arxiv.org/pdf/1610.03022.pdf, \n",
    "1. No conv(3x3)/2 used \n",
    "2. Added an extra dense layer of 256 units\n",
    "\n",
    "83.8% on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metal_geek/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(\"../libs\")\n",
    "from classification import input_data\n",
    "from classification import models\n",
    "from classification import trainer\n",
    "from classification import freeze\n",
    "import hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the data folder to use the required data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags=tf.app.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags=tf.app.flags\n",
    "#Important Directories\n",
    "flags.DEFINE_string('data_dir','../data/raw','Train Data Folder')\n",
    "flags.DEFINE_string('summaries_dir','../summaries','Summaries Folder')\n",
    "flags.DEFINE_string('train_dir','../logs&checkpoint','Directory to write event logs and checkpoint')\n",
    "flags.DEFINE_string('models_dir','../models','Models Folder')\n",
    "#Task Specific Parameters\n",
    "flags.DEFINE_string('wanted_words','yes,no,up,down,left,right,on,off,stop,go','Wanted Words')\n",
    "flags.DEFINE_float('validation_percentage',10,'Validation Percentage')\n",
    "flags.DEFINE_float('testing_percentage',10,'Testing Percentage')\n",
    "flags.DEFINE_integer('sample_rate',16000,'Sample Rate')\n",
    "flags.DEFINE_integer('clip_duration_ms',1000,'Clip Duration in ms')\n",
    "flags.DEFINE_float('window_size_ms',30,'How long each spectogram timeslice is')\n",
    "flags.DEFINE_float('window_stride_ms',10.0,'How far to move in time between frequency windows.')\n",
    "flags.DEFINE_integer('dct_coefficient_count',40,'How many bins to use for the MFCC fingerprint')\n",
    "flags.DEFINE_float('time_shift_ms',100.0,'Range to randomly shift the training audio by in time.')\n",
    "\n",
    "FLAGS=flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture='convlstm'\n",
    "start_checkpoint=None\n",
    "logging_interval=10\n",
    "eval_step_interval=1000\n",
    "save_step_interval=100000\n",
    "silence_percentage=10.0\n",
    "unknown_percentage=10.0\n",
    "background_frequency=0.8\n",
    "background_volume=0.1\n",
    "train_steps='3500' #Declare  the training steps for which the learning rates will be used\n",
    "learning_rate='0.0001'\n",
    "batch_size=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-f44c564acc13>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f44c564acc13>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Loldef resCONVLSTM(inputs, model_settings, is_training, name='',conv_lstm_filter_size=4):\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def resCONVLSTM(inputs, model_settings, is_training, name='',conv_lstm_filter_size=4):\n",
    "    \"\"\"Creates a Residual ConvLSTM as in https://arxiv.org/abs/1607.06450.\n",
    "        1-D Conv on feature, unidirectional rnn\n",
    "        \n",
    "    \"\"\"\n",
    "    with(tf.variable_scope('resCONVLSTM_%s' % name)):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        input_frequency_size = model_settings['dct_coefficient_count']\n",
    "        input_time_size = model_settings['spectrogram_length']\n",
    "        input_shape = [input_frequency_size, 1]\n",
    "        conv1 = tf.contrib.rnn.ConvLSTMCell(1, input_shape, 1, [conv_lstm_filter_size], name='conv1')\n",
    "        conv2 = tf.contrib.rnn.ConvLSTMCell(1, input_shape, 1, [conv_lstm_filter_size], name='conv2')\n",
    "        # First ConvLSTM\n",
    "        initial_conv1 = conv1.zero_state(batch_size, dtype=tf.float32)\n",
    "        initial_conv2 = conv2.zero_state(batch_size, dtype=tf.float32)\n",
    "        conv1_o, _ = tf.nn.dynamic_rnn(conv1, inputs, initial_state=initial_conv1)\n",
    "        bn1 = tf.layers.batch_normalization(inputs, axis=2, training=is_training)\n",
    "        bn1_relu = tf.nn.relu(bn1)\n",
    "        conv2_o, _ = tf.nn.dynamic_rnn(conv2, bn1_relu, initial_state=initial_conv2)\n",
    "        bn2 = tf.layers.batch_normalization(conv2_o, axis=2, training=is_training)\n",
    "        residual = tf.add(bn2, inputs)\n",
    "        output_relu = tf.nn.relu(residual)\n",
    "        return output_relu\n",
    "\n",
    "\n",
    "def create_multilayer_convlstm_model(fingerprint_input, model_settings, is_training,conv_lstm_filter_size=4,lstm_size=256,dense_size=256):\n",
    "    \"\"\"\n",
    "        Creates a Multilayer ConvLSTM Model Followed by a linear layer and softmax activation function\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    batch_size = tf.shape(fingerprint_input)[0]\n",
    "    input_frequency_size = model_settings['dct_coefficient_count']\n",
    "    input_time_size = model_settings['spectrogram_length']\n",
    "    fingerprint_4d = tf.reshape(fingerprint_input,\n",
    "                                [-1, input_time_size, input_frequency_size, 1])\n",
    "\n",
    "    # Layer1 resCONVLSTMs\n",
    "    resCONVLSTM1 = resCONVLSTM(fingerprint_4d, model_settings, is_training, '1',conv_lstm_filter_size)\n",
    "    resCONVLSTM2 = resCONVLSTM(resCONVLSTM1, model_settings, is_training, '2',conv_lstm_filter_size)\n",
    "    resCONVLSTM3 = resCONVLSTM(resCONVLSTM2, model_settings, is_training, '3',conv_lstm_filter_size)\n",
    "    resCONVLSTM4= resCONVLSTM(resCONVLSTM3,model_settings,is_training,'4',conv_lstm_filter_size)\n",
    "    resCONVLSTM4=tf.reshape(resCONVLSTM4,[-1, input_time_size, input_frequency_size])\n",
    "    with tf.variable_scope('lstm1'):\n",
    "        lstm_cell1=tf.contrib.rnn.LSTMCell(num_units=lstm_size,num_proj=input_frequency_size)\n",
    "        initial_lstm1=lstm_cell1.zero_state(batch_size,dtype=tf.float32)\n",
    "        lstm1_o,_=tf.nn.dynamic_rnn(lstm_cell1,resCONVLSTM4,initial_state=initial_lstm1)\n",
    "        lstm1_o=tf.reshape(lstm1_o,[-1, input_time_size, input_frequency_size,1 ])\n",
    "        nin1_o=tf.layers.conv2d(lstm1_o,1,[1,1],name='nin1')\n",
    "        bn1=tf.layers.batch_normalization(nin1_o, axis=2, training=is_training)\n",
    "        bn1=tf.reshape(bn1,[-1,input_time_size,input_frequency_size])\n",
    "    with tf.variable_scope('lstm2'):\n",
    "        lstm_cell2=tf.contrib.rnn.LSTMCell(num_units=lstm_size,num_proj=input_frequency_size)\n",
    "        initial_lstm2=lstm_cell1.zero_state(batch_size,dtype=tf.float32)\n",
    "        lstm2_o, _ = tf.nn.dynamic_rnn(lstm_cell2, bn1, initial_state=initial_lstm2)\n",
    "        lstm2_o = tf.reshape(lstm2_o, [-1, input_time_size, input_frequency_size, 1])\n",
    "        nin2_o = tf.layers.conv2d(lstm2_o, 1, [1, 1], name='nin1')\n",
    "        bn2 = tf.layers.batch_normalization(nin2_o, axis=2, training=is_training)\n",
    "        bn2=tf.reshape(bn2,[-1,input_time_size,input_frequency_size])\n",
    "\n",
    "    # LSTM Layer Final\n",
    "    with tf.variable_scope('lstm3'):\n",
    "        lstm_cell3=tf.contrib.rnn.LSTMCell(num_units=lstm_size,num_proj=input_frequency_size)\n",
    "        initial_lstm3=lstm_cell1.zero_state(batch_size,dtype=tf.float32)\n",
    "        lstm3_o, _ = tf.nn.dynamic_rnn(lstm_cell3, bn2, initial_state=initial_lstm3)\n",
    "        lstm3_o = tf.reshape(lstm3_o, [-1, input_time_size, input_frequency_size, 1])\n",
    "\n",
    "\n",
    "    # Final FC for classification\n",
    "    reshaped_layer = tf.reshape(lstm3_o,\n",
    "                                 [-1, input_time_size * input_frequency_size])\n",
    "\n",
    "    # Dropout\n",
    "    if is_training:\n",
    "        reshaped_layer = tf.nn.dropout(reshaped_layer, keep_prob=dropout_prob)\n",
    "\n",
    "\n",
    "    prefinal_dense=tf.nn.relu(tf.layers.dense(reshaped_layer,dense_size))\n",
    "\n",
    "    if is_training:\n",
    "        prefinal_dense=tf.nn.dropout(prefinal_dense,keep_prob=dropout_prob)\n",
    "    # Final Layer\n",
    "\n",
    "    label_count = model_settings['label_count']\n",
    "\n",
    "    final_fc_weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [dense_size, label_count], stddev=0.01))\n",
    "    final_fc_bias = tf.Variable(tf.zeros([label_count]))\n",
    "    final_fc = tf.matmul(prefinal_dense, final_fc_weights) + final_fc_bias\n",
    "    if is_training:\n",
    "        return final_fc, dropout_prob\n",
    "    else:\n",
    "        return final_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(args):\n",
    "    conv_lstm_filter_size=int(args['conv_lstm_filter_size'])\n",
    "    lstm_size=int(args['lstm_size'])\n",
    "    dense_size=int(args['dense_size'])\n",
    "    dropout=args['dropout']\n",
    "    print('Eval Start')\n",
    "    print(conv_lstm_filter_size,lstm_size,dense_size,dropout)\n",
    "    tf.reset_default_graph()\n",
    "    train_dir=os.path.join(FLAGS.data_dir,'train','audio')\n",
    "    model_settings = models.prepare_model_settings(\n",
    "      len(input_data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "      FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "      FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)\n",
    "    audio_processor = input_data.AudioProcessor(\n",
    "      train_dir, silence_percentage, unknown_percentage,\n",
    "      FLAGS.wanted_words.split(','), FLAGS.validation_percentage,\n",
    "      FLAGS.testing_percentage, model_settings)\n",
    "    \n",
    "    def get_train_data(args):\n",
    "        sess=args\n",
    "        time_shift_samples = int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000)\n",
    "        train_fingerprints, train_ground_truth = audio_processor.get_data(\n",
    "            batch_size, 0, model_settings,background_frequency,\n",
    "            background_volume, time_shift_samples, 'training', sess)\n",
    "        return train_fingerprints,train_ground_truth\n",
    "    \n",
    "    def get_val_data(args):\n",
    "        '''\n",
    "        Input: (sess,offset)\n",
    "        '''\n",
    "        sess,i=args\n",
    "        validation_fingerprints, validation_ground_truth = (\n",
    "                audio_processor.get_data(batch_size, i, model_settings, 0.0,\n",
    "                                         0.0, 0, 'validation', sess))\n",
    "        return validation_fingerprints,validation_ground_truth\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Placeholders\n",
    "        fingerprint_size = model_settings['fingerprint_size']\n",
    "        label_count = model_settings['label_count']\n",
    "        fingerprint_input = tf.placeholder(\n",
    "          tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "        ground_truth_input = tf.placeholder(\n",
    "          tf.float32, [None, label_count], name='groundtruth_input')\n",
    "        set_size = audio_processor.set_size('validation')\n",
    "        label_count = model_settings['label_count']\n",
    "        # Create Model\n",
    "\n",
    "        logits, dropout_prob = create_multilayer_convlstm_model(\n",
    "          fingerprint_input,\n",
    "          model_settings,\n",
    "          True,\n",
    "          conv_lstm_filter_size,\n",
    "          lstm_size,\n",
    "          dense_size)\n",
    "        #Start Training\n",
    "        extra_args=(dropout_prob,label_count,batch_size,set_size)\n",
    "        val_acc=trainer.train(sess,logits,fingerprint_input,ground_truth_input,get_train_data,\n",
    "                      get_val_data,train_steps,learning_rate,eval_step_interval, logging_interval=logging_interval,\n",
    "                      start_checkpoint=None,checkpoint_interval=None,\n",
    "                      model_name=model_architecture,train_dir=None,\n",
    "                      summaries_dir=None,dropout=dropout,args=extra_args)\n",
    "    return 1-val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "        'conv_lstm_filter_size': hyperopt.hp.uniform('conv_lstm_filter_size', 4,20),\n",
    "        'lstm_size': hyperopt.hp.uniform('lstm_size', 128,756),\n",
    "        'dense_size': hyperopt.hp.uniform('dense_size', 128,756),\n",
    "        'dropout':hyperopt.hp.uniform('dropout',0.3,1)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model=hyperopt.fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best selected Hyperparameters')\n",
    "print(hyperopt.space_eval(space, best_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
